{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "971e2519",
   "metadata": {},
   "source": [
    "# CHAPTER-6 Handling Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac255ac9",
   "metadata": {},
   "source": [
    "Stratergies for transforming text into information-rich features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf43c628",
   "metadata": {},
   "source": [
    "## 6.1 Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4dde17",
   "metadata": {},
   "source": [
    "Cleanig an unstructured data. Most basic operations are by using python string operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "894b3a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a text\n",
    "\n",
    "text_data = [\" Interrobang. By Aishwarya Henriette   \",\n",
    "            \"Parking And Going. By Karl Gautier\",\n",
    "            \" Today Is The night. By Jarek Prakash \"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "709bc2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip Whitespaces\n",
    "strip_whitespace = [ string.strip() for string in text_data ]\n",
    "\n",
    "# display text\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc5bf4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang  By Aishwarya Henriette',\n",
       " 'Parking And Going  By Karl Gautier',\n",
       " 'Today Is The night  By Jarek Prakash']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove periods\n",
    "\n",
    "remove_periods = [string.replace(\".\",\" \") for string in strip_whitespace]\n",
    "\n",
    "# display text\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05e6bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG  BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING  BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT  BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating and applying a custom transformation function\n",
    "\n",
    "def cpitalize(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "[cpitalize(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd8b4e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cpitalizer(strn):\n",
    "#     return strn.upper()\n",
    "\n",
    "# [cpitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32ba2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use regualr expressions to make powerful string operations:\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1ed0cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX  XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX  XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX  XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace letters with X function\n",
    "\n",
    "def replace_with_X(string: str) -> str:\n",
    "    return re.sub(r\"[a-zA-Z]\",\"X\",string)\n",
    "\n",
    "[replace_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5c2588",
   "metadata": {},
   "source": [
    "## 6.2 Parsing and Cleaning HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae2a9e3",
   "metadata": {},
   "source": [
    "Extracting text data from HTML elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "674ac620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing BeautifulSoup library\n",
    "# BeautifulSoup - for scrapping HTML pages\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a073a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating HTML code\n",
    "\n",
    "html = \"\"\" <div class ='fullName'><span style = 'font-weight:bold'>Masego</span> Azra</div> \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f0823158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing HTML\n",
    "\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# lxml is a HTML parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe314d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# finding a particular div and showing the text in it\n",
    "\n",
    "soup.find(\"div\", {\"class\" : \"fullName\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82630b",
   "metadata": {},
   "source": [
    "## 6.3 Removing Punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d3aa6",
   "metadata": {},
   "source": [
    "Removing punctuation from feature of text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed46c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "import unicodedata\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd6a5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text\n",
    "\n",
    "text = {'Hi!!!! I. Love. This. Song....',\n",
    "       '10000% Agree!!!! #LoveIT',\n",
    "       'Right?!?!'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6fb21fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary of punctuation charecters\n",
    "\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ec36704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', 'Right', '10000 Agree LoveIT']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[string.translate(punctuation) for string in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda9a50",
   "metadata": {},
   "source": [
    "## 6.4 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa4f75f",
   "metadata": {},
   "source": [
    "Breaking text into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "99ab874c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing word tokenizer librrary \n",
    "# NLTK: Natural Language ToolKit\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c4d5e48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import nltk'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# have to be downloaded to run word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "671dbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text\n",
    "\n",
    "tokenize_word = \"The science of today is the technology of tomorrow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8a5264a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing words\n",
    "\n",
    "word_tokenize(tokenize_word)\n",
    "\n",
    "# word tokenizing is the most common and first step in cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ee34b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also tokenize sentences\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ad03cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a text\n",
    "\n",
    "tokenize_sentence = \"The science of today is the technology of tomorrow. Tomorrow is today. Today is now :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8afe0aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The science of today is the technology of tomorrow.',\n",
       " 'Tomorrow is today.',\n",
       " 'Today is now :)']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizing the sentence\n",
    "\n",
    "sent_tokenize(tokenize_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01e3933",
   "metadata": {},
   "source": [
    "## 6.5 Removing stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8cf30",
   "metadata": {},
   "source": [
    "Removing less information words from tokenized text data. By using NLTK's stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "797d9335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the library\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "20d064c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when working on the stopwords firt time we have to download set of stopwords\n",
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a9002b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_words = ['The', 'science', 'of', 'today', 'is', 'the', 'technology', 'of', 'tomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "887fc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d8bf0415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show few stopwords\n",
    "\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f7e130eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'science', 'today', 'technology', 'tomorrow']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a272918",
   "metadata": {},
   "source": [
    "## 6.6 Stemming words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bfc079",
   "metadata": {},
   "source": [
    "Converting tokenized words into root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "72568713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1a107cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating word tokens\n",
    "\n",
    "tokenized_words = ['i', 'am', 'humbled', 'by', 'this', 'traditional', 'meeting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "797f6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating stemmer\n",
    "\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5b9c432e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying stemmer \n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47260125",
   "metadata": {},
   "source": [
    "## 6.7 Tagging Parts of Speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6647d",
   "metadata": {},
   "source": [
    "Tagging text data with its Parts of Speech. Using pretrained model of NLTK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6555c74",
   "metadata": {},
   "source": [
    "Text tagging is nothing but labeling words in a text with thier grammatical categories, like nouns, verbs...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0621bc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b2d5ab63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1dabc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text data\n",
    "\n",
    "text_data = \"Chris loved outdoor running\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ded896ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tagged =  pos_tag(word_tokenize(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8ea79bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_tagged\n",
    "\n",
    "# output is a list of tuples with the word and the tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d286a7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can filter the words  in based on parts of speech\n",
    "\n",
    "[word for word, tag in text_tagged if tag in ['NN', 'NNS', 'NNP', 'NNPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "eedb0b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting sentences into features based on thier individual parts of speech(like tweets), \n",
    "# feature with 1 if a proper noun is present and 0 otherwise\n",
    "\n",
    "#creating text\n",
    "\n",
    "tweets = [\"I am eating a burrito for breakfast\",\n",
    "         \"Political science is an amazing field\",\n",
    "         \"San Francisco is an awesome city\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "03da343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a empty list\n",
    "\n",
    "tagged_tweets = []\n",
    "\n",
    "# tagging each word for each tweet\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = pos_tag(word_tokenize(tweet))\n",
    "    tagged_tweets.append([tag for word, tag in tweet_tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "f1e3c255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "a8f3678a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using one-hot encoding to convert tags into features\n",
    "\n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagged_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5336159b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using classes_ we can check each feature is a parts of speech\n",
    "\n",
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b36d59",
   "metadata": {},
   "source": [
    "Tagged Corpus: Its a collection of text data where each word is associated with its Parts-Of-Speech tag(POS tagging). This is useful for training and evaluating POS tagging models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c35e1a",
   "metadata": {},
   "source": [
    "Corpus used - Brown Corpus\n",
    "\n",
    "Tagger used - n-gram tagger\n",
    "\n",
    "n is the no of previous words we see for predicting. first we take previous 2 words using TrigramTagger, if 2 words are not present we back off and take one previous word using BigramTagger.\n",
    "\n",
    "To check the accuracy of our tagger, we split our text data into 2 parts, train on first part and test on second part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d12e5c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "77d54c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "016118c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text from Brown Corpus, broken into sentences\n",
    "\n",
    "sentences = brown.tagged_sents(categories = 'news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cb32e0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4623"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c2b1fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting such that, 4000 sentences for training and 623 for testing\n",
    "\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4a0e41e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating backoff taggers\n",
    "\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff = unigram)\n",
    "trigram = TrigramTagger(train, backoff = bigram)\n",
    "\n",
    "# if 3 previous words do not present then TrigramTagger fallsback to BigramTagger, \n",
    "# if 2 previous words do not present then BigramTagger fallsback to UnigramTagger,\n",
    "# uses that word without considering other words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c20ad84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_14268\\3110557013.py:3: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram.evaluate(test)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show accuracy\n",
    "\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b7950",
   "metadata": {},
   "source": [
    "## 6.8 Encoding Text as a Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd269c",
   "metadata": {},
   "source": [
    "Creating set of Features to find the number of times a word is present in a text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "848399a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ed0f5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best',\n",
    "                     'India beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "45e1e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating bag of words feature matrix\n",
    "\n",
    "count = CountVectorizer()\n",
    "\n",
    "bag_of_words = count.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "dbe2acba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature matrix\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "650c508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'india', 'is', 'love', 'sweden'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4781c607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 2, 0, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [1, 0, 1, 0, 1, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output of above is a sparse array we can use toarray to view the matrix\n",
    "\n",
    "bag_of_words.toarray()\n",
    "\n",
    "# from the output we can observe that brazil feature occurs twice so the count below is 2,\n",
    "# each word above represents a feature, \n",
    "# for large text data the resulting matrix can contain thousands of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffdcbdf",
   "metadata": {},
   "source": [
    "In many cases most of the words do not occur again, resulting in a feature matrix of too many zeros which takes up lot of memory. We can use sparse matrix which only stores nonzero values. CountVectorizer outputs the sparse matrix by default.\n",
    "\n",
    "In default case every feature is a word instead we can take combination of words called 2-gram or 3 gram.\n",
    "\n",
    "'ngram_range' sets minimum and maximum size of grams, for example, (1,2) returns all 1-grams and 2-grams, (2,3) returns all 2-grams and 3-grams.\n",
    "\n",
    "And we can remove filler words using 'stop_words'.\n",
    "\n",
    "We can restrict the words using 'vocabulary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bfecd42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature matrix\n",
    "\n",
    "coungt_2gram = CountVectorizer(ngram_range = (1,2),\n",
    "                              stop_words = 'english',\n",
    "                              vocabulary = ['brazil'])\n",
    "\n",
    "bag = coungt_2gram.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "20b816a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show feature matrix\n",
    "\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "71830cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brazil': 0}"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 1-grams and 2-grams\n",
    "\n",
    "coungt_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "c083499a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating feature matrix\n",
    "\n",
    "coungt_2gram = CountVectorizer(ngram_range = (1,1),\n",
    "                              stop_words = 'english')\n",
    "\n",
    "bag = coungt_2gram.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "07d9b0e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, 0, 1, 0],\n",
       "       [0, 1, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show feature matrix\n",
    "\n",
    "bag.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4f6c9ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 4, 'brazil': 2, 'sweden': 5, 'best': 1, 'india': 3, 'beats': 0}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View 1-grams\n",
    "\n",
    "coungt_2gram.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d25fe",
   "metadata": {},
   "source": [
    "## 6.9 Weighing Word Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c365c46",
   "metadata": {},
   "source": [
    "Bag of words based on thier importance.\n",
    "\n",
    "This is done by comparing the frequency of the word in a document with frequency of the word in all other documents using 'term frequency-inverse document frequency'(tf-idf). \n",
    "\n",
    "scikit-learn provides TfidfVectorizer for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "abdaaa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "cb802e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating text data\n",
    "\n",
    "text_data = np.array(['I love Brazil. Brazil!',\n",
    "                     'Sweden is best',\n",
    "                     'India beats both'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c819c104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating tf-idf feature matrix\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "09950ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "e3ef6706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.89442719, 0.        ,\n",
       "        0.        , 0.4472136 , 0.        ],\n",
       "       [0.        , 0.57735027, 0.        , 0.        , 0.        ,\n",
       "        0.57735027, 0.        , 0.57735027],\n",
       "       [0.57735027, 0.        , 0.57735027, 0.        , 0.57735027,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray()\n",
    "\n",
    "the output returned here is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0df1b897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 8)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3776d546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 6,\n",
       " 'brazil': 3,\n",
       " 'sweden': 7,\n",
       " 'is': 5,\n",
       " 'best': 1,\n",
       " 'india': 4,\n",
       " 'beats': 0,\n",
       " 'both': 2}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "93f96b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['beats', 'best', 'both', 'brazil', 'india', 'is', 'love', 'sweden'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d098703",
   "metadata": {},
   "source": [
    "'tf' is the no of times a word occurs in a document and 'idf' is calculated with the number of documents and number of times the word occured in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e025b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "50ee43b9",
   "metadata": {},
   "source": [
    "Tag - Part of speech\n",
    "================\n",
    "\n",
    "    NNP: Proper noun, singular\n",
    "    NN: Noun, singular or mass\n",
    "    RB: Adverb\n",
    "    VBD: Verb, past tense\n",
    "    VBG: Verb, gerund or present participle\n",
    "    JJ: Adjective\n",
    "    PRP: Personal pronoun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb42b2",
   "metadata": {},
   "source": [
    "KEYPOINTS:\n",
    "\n",
    "    NLTK stopwords assumes all tokenized words are lowercased\n",
    "    \n",
    "    Corpus: Corpus in NLP is a large ans tructures text in particular language. Serves as a resource for language-related algorithms and models.\n",
    "    \n",
    "    Tagged Corpus: Its a collection of text data where each word is associated with its Parts-Of-Speech tag(POS tagging). This is useful for training and evaluating POS tagging models.\n",
    "    \n",
    "    Brown Corpus: Collection of test samples from various sources representing American english.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af4da1e",
   "metadata": {},
   "source": [
    " KEYWORDS:\n",
    "    \n",
    "     BeautifulSoup: A python library for scrapping HTML live websites.\n",
    "     nltk: Natural Language ToolKit is a python library for text manipulation.\n",
    "     PorterStemmer: Stemming algorithm to remove affixes keeping the stem.\n",
    "     pos_tag: Parts-Of-Speech tagging(pos_tag). where each word is tagged with its parts of speech\n",
    "     UnigramTagger: Is a type of POS tagging in which it assigns tags based solely on its occurencies, without considering the context of surrounding words\n",
    "     BigramTagger: Is a type of POS tagging in which it assigns tags based on the probability of previous 2 words.\n",
    "     TrigramTagger: Is a type of POS tagging in which it assigns tags based on the probability of previous 3 words.\n",
    "     CountVectorizer: To convert text-documents to a feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61b17d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
