{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b596161",
   "metadata": {},
   "source": [
    "# CHAPTER-9 Dimensionality Reduction Using Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef50987",
   "metadata": {},
   "source": [
    "The goal of dimensionality reduction is to transform our set of features such that the original feature is bigger than the transformed feature, while still keeping the underlying information.\n",
    "\n",
    "With small loss in our data we reduce the no of features to generate high quality predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c6cdd",
   "metadata": {},
   "source": [
    "## 9.1 Reducing Features using Principle Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa66ae5e",
   "metadata": {},
   "source": [
    "Reducing the number of features while retaining the variance in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edcaa0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "# PCA - Principle Component Analysis\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0122cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16ec39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standardizing the feature matrix\n",
    "\n",
    "feature = StandardScaler().fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcd77693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating PCA that will retain 99% of the variance\n",
    "\n",
    "pca = PCA(n_components = 0.99, whiten = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "617d927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct PCA\n",
    "\n",
    "feature_pca = pca.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64d1ce0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features:  64\n",
      "Reduced number of features:  54\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "\n",
    "print(\"Original number of features: \",feature.shape[1])\n",
    "print(\"Reduced number of features: \",feature_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afaf56c",
   "metadata": {},
   "source": [
    "Principle Component Analysis is a popular dimensionality reduction technique.\n",
    "\n",
    "It is a unsupervised technique, it does not use information from target vector instead considers only feature matrix.\n",
    "\n",
    "n_components: has 2 opeartions, if the value is greater than 1,\n",
    "    \n",
    "    it will return that many features.\n",
    "    if the value is between 0 and 1 it will return minimum amount of features that retain that much variance. 0.95 and 0.99 are most common values used.\n",
    "    \n",
    "\"whiten = true\": transforms the values of each principle components such that they have zero mean and unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e6383",
   "metadata": {},
   "source": [
    "## 9.2 Reducing Features when data is Linearly Inseperable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed15a3e",
   "metadata": {},
   "source": [
    "We use extension of PCA that uses kernels to allow for non-linear dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19346b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d13e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating linearly inseperable data\n",
    "\n",
    "features, _ = make_circles(n_samples = 1000, random_state = 1, noise = 0.1, factor = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd037597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply kernel PCA with radius basis function(RBF) kernal\n",
    "\n",
    "kpca = KernelPCA(kernel = \"rbf\", gamma = 15, n_components = 1)\n",
    "features_kpca = kpca.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ba368a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features:  2\n",
      "Reduced number of features:  1\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "\n",
    "print(\"Original number of features: \",features.shape[1])\n",
    "print(\"Reduced number of features: \",features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0208ea",
   "metadata": {},
   "source": [
    "Kernel PCA can reduce the dimensions and also can make data linearly inseperable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3273b1",
   "metadata": {},
   "source": [
    "## 9.3 Reducing features by Maximizing Class Sperability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969e88dd",
   "metadata": {},
   "source": [
    "Reducing the features used by a classifier, Linear Discriminant Analysis(LDA) to project features onto component axes that increases the seperation of axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0856cc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769441ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Iris flower dataset\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7883d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating and running LDA\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components = 1)\n",
    "\n",
    "# transform the features\n",
    "\n",
    "features_lda = lda.fit(features, target).transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a30bd654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features:  4\n",
      "Reduced number of features:  1\n"
     ]
    }
   ],
   "source": [
    "# printing the number of features\n",
    "\n",
    "print(\"Original number of features: \", features.shape[1])\n",
    "print(\"Reduced number of features: \", features_lda.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcd2eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.9912126])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.explained_variance_ratio_\n",
    "\n",
    "# amount of varianced explained by each component, \n",
    "# here a single component explained over 99% of the variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c4630d",
   "metadata": {},
   "source": [
    "LDA is a classification which is also known for dimensionality reduction.\n",
    "\n",
    "we can run LDA to return the ratio of variance explained for each component and calculate how many components are required to get above some threshold of explained variance(0.95 or 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "412045ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and running LDA\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "features_lda = lda.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5fc24e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# array of explained variance\n",
    "\n",
    "lda_var_ratio = lda.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff5f9e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function\n",
    "\n",
    "def select_n_components(var_ratio, goal_var: float) -> int:\n",
    "    \n",
    "    total_variance = 0.0 # setting initial variance\n",
    "    n_components = 0 # initial number of features\n",
    "    \n",
    "    # for explained variance of each feature\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        # adding explained variance to total variance\n",
    "        total_variance += explained_variance\n",
    "        \n",
    "        # adding 1 to no of components\n",
    "        n_components += 1\n",
    "        \n",
    "        # If we reach  our goal level of explained varince\n",
    "        if total_variance >= goal_var:\n",
    "            break # end of loop\n",
    "    \n",
    "    # return the number of components\n",
    "    return n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86953d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run function\n",
    "\n",
    "select_n_components(lda_var_ratio, 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570483ce",
   "metadata": {},
   "source": [
    "## 9.4 Reducing Features Using Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea79a6f",
   "metadata": {},
   "source": [
    "Reducing the dimensions of a feature matrix of nonnegative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c032bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the libraries\n",
    "# NMF: Nonnegatice Matrix Factorization\n",
    "\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40555f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ede0d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load feature matrix\n",
    "\n",
    "features = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e75a1d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1665: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# creating, fit and applying NMF\n",
    "\n",
    "nmf = NMF(n_components = 10, random_state = 1)\n",
    "features_nmf = nmf.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b760f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orginal number of features:  64\n",
      "Reduced number of features:  10\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "\n",
    "print(\"Orginal number of features: \", features.shape[1])\n",
    "print(\"Reduced number of features: \", features_nmf.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21a7d4",
   "metadata": {},
   "source": [
    "NMF can reduce the dimensioanlity because in matrix multiplication the two factors can have fewer dimensions than the product matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d63d454",
   "metadata": {},
   "source": [
    "## 9.5 Reducing Features on Sparse Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e887657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "# using Truncated Singular Valued Decomposition\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a993be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e14151d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the feature matrix\n",
    "\n",
    "features = StandardScaler().fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5acc9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sparse matrix\n",
    "\n",
    "features_sparse = csr_matrix(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e2fe958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Truncated Singular Valued Decomposition\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65b651c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conducting TSVD on sparse matrix\n",
    "\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91aedf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of features:  64\n",
      "Reduced number of features:  10\n"
     ]
    }
   ],
   "source": [
    "# show results\n",
    "\n",
    "print(\"Original number of features: \", features_sparse.shape[1])\n",
    "print(\"Reduced number of features: \", features_sparse_tsvd.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4fc16",
   "metadata": {},
   "source": [
    "TSVD is similar to PCA, but unlike PCA TSVD works on sparse feature matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d02697fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3003938538086374"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First three components explain 30% of original data's variance\n",
    "\n",
    "tsvd.explained_variance_ratio_[0:3].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "434552bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating and running TSVD with one less number of features\n",
    "\n",
    "tsvd = TruncatedSVD(n_components = features_sparse.shape[1]-1)\n",
    "features_tsvd = tsvd.fit(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6fccf825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of explained variances\n",
    "\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6695135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function\n",
    "\n",
    "def select_n_components(var_ratio, goal_var):\n",
    "    \n",
    "    total_variance = 0.0 # setting initial variance\n",
    "    n_components = 0 # initial number of features\n",
    "    \n",
    "    # explained variance for each feature\n",
    "    for explained_variance in var_ratio:\n",
    "        \n",
    "        total_variance += explained_variance # adding explained variance to total variance\n",
    "        \n",
    "        n_components += 1 # adding 1 to the number of components\n",
    "        \n",
    "        # if we reach our goal level of explained variance\n",
    "        if total_variance >= goal_var:\n",
    "            \n",
    "            break # end the loop\n",
    "            \n",
    "    return n_components # return number of components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95e6db58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run function\n",
    "\n",
    "select_n_components(tsvd_var_ratios, 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aac743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
