{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2355a832",
   "metadata": {},
   "source": [
    "# CHAPTER - 13: Linear Regression\n",
    "\n",
    "Used when target vector is a quantitative value.\n",
    "\n",
    "## 13.1 Fitting a line\n",
    "\n",
    "To train a model that represents a linear relationship between feature and target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77d7485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4787c214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data with only two features\n",
    "\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:2]\n",
    "target = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46771fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating linear regression\n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0cd02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the linear regression\n",
    "\n",
    "model = regression.fit(features, target)\n",
    "\n",
    "# print(\"Feat\", features.shape)\n",
    "# print(\"target\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea694e0",
   "metadata": {},
   "source": [
    "Linear Regression assumes that relationship between features and target is linear i.e., the effect of features on the target is constant.\n",
    "\n",
    "here we are taking only 2 features, so the linear model looks like:\n",
    "\n",
    "y_hat = b0 + b1 x1 + b2 x2 + e\n",
    "\n",
    "y_hat: target\n",
    "\n",
    "xi: data for single feature\n",
    "\n",
    "b1, b2: coefficients identified by fitting the model\n",
    "\n",
    "b0: bias/intercept\n",
    "\n",
    "e: error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "395576e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.10189032759082695"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view the intercept\n",
    "\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfb4d3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43169191, 0.01744134])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view the coefficients\n",
    "\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5876714a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4526.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target value is the median of the dataset, so the price of the first home from dataset is:\n",
    "\n",
    "target[0] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ce82d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4207.126263821179"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# value of the first house prediction using the model\n",
    "\n",
    "model.predict(features)[0]*1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a113159b",
   "metadata": {},
   "source": [
    "The model created here is just $319 off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460c6b3",
   "metadata": {},
   "source": [
    "## 13.2 Handling Interactive Effects\n",
    "A feature whose effect on the target variable depends on another feature\n",
    "\n",
    "Craeting an interaction term to capture that dependence using scikit-learn's PolynomialFeatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d0473396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0348d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data with only 2 features\n",
    "\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:2]\n",
    "target = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "de938f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reating interaction term\n",
    "interaction = PolynomialFeatures(\n",
    "degree = 3, include_bias = False, interaction_only = True)\n",
    "features_interaction = interaction.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20f77fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f808c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the linear regression\n",
    "\n",
    "model = regression.fit(features_interaction, target)\n",
    "# print(\"Feat\", features_interaction.shape)\n",
    "# print(\"target\", target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8cfe4b",
   "metadata": {},
   "source": [
    "Linear regression assumes that, the relationship between features and the target vector is approximately linear, i.e., the effect of features on the target vector is constant. Th equation with features looks like:\n",
    "\n",
    "y^ = b0^ + b1^ x1 + b2^ x2 + e\n",
    "\n",
    "y^: target vector\n",
    "\n",
    "xi: data of single feature\n",
    "\n",
    "b1^, b2^: coefficients\n",
    "\n",
    "e: error\n",
    "\n",
    "b0^: bias/intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66f0bd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.3252, 41.    ])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to view the feature values for first observation\n",
    "\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fabd9150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create an interactive term we multiply those 2 values together for every observations\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b4dd9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_term = np.multiply(features[:,0], features[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "099a95cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341.33320000000003"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# interaction term for the first observation\n",
    "\n",
    "interaction_term[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d4e962",
   "metadata": {},
   "source": [
    "PolynomialFeatures of scikit-learn is used to create interaction terms, then we can use model selection strategies to identify the combination of features and interaction terms that produce the best model.\n",
    "\n",
    "There are 3 important parameters we must set for PolynomialFeatures:- \n",
    "1) interaction_only = True: tells the PolynomialFeatures to only return interaction terms.\n",
    "2) include_bias = False: by default PolynomialFeatures will ad  a feature containing ones called a bias.\n",
    "3) degree: determines maximum no.of features to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8e45cffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3252,  41.    , 341.3332])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# values of first observation\n",
    "\n",
    "features_interaction[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628d970b",
   "metadata": {},
   "source": [
    "## 13.3 Fitting a Nonlinear Relationship\n",
    "\n",
    "Creating a Polynomial regression by including a polynomial features in a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07600e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9334dc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data with one feature\n",
    "\n",
    "california = fetch_california_housing()\n",
    "features = california.data[:,0:1]\n",
    "target = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3f175688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating polynomial features x^2 and x^3\n",
    "\n",
    "polynomial = PolynomialFeatures(degree = 3, include_bias = False)\n",
    "features_polynomial = polynomial.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b477713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create linear regression\n",
    "\n",
    "regression = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0df05e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the linear regression\n",
    "\n",
    "model = regression.fit(features_polynomial, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9c88fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first degree term: [8.3252]\n",
      "second degree term: [69.30895504]\n",
      "third degree term: [577.0109125]\n"
     ]
    }
   ],
   "source": [
    "# only one observation, 1st observation\n",
    "\n",
    "print(\"first degree term:\",features[0])\n",
    "\n",
    "# to create a polynomial feature, we can rasie the power by second degree, this forms a new feature\n",
    "\n",
    "print(\"second degree term:\",features[0]**2)\n",
    "\n",
    "# we can increase the degree and add one more feature\n",
    "\n",
    "print(\"third degree term:\",features[0]**3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2420ee8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  8.3252    ,  69.30895504, 577.0109125 ])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all these can be included to form a single feature matrix and then running a feature matrix\n",
    "\n",
    "features_polynomial[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f1eb2",
   "metadata": {},
   "source": [
    "## 13.4 Reducing Variance with Regularization\n",
    "\n",
    "To reduce variance in Linear Regression model, using a learning algorithm that includes a shrinkage penalty, also called Regularization, like Ridge regression and Lasso regression:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d7a263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "53de2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading california dataset\n",
    "\n",
    "california = fetch_california_housing()\n",
    "features = california.data\n",
    "target = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a4aff010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4eac86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating ridge regression with an alpha value\n",
    "\n",
    "regression = Ridge(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e473c762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the linear regression\n",
    "\n",
    "model = regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e45d51",
   "metadata": {},
   "source": [
    "As we know Linear regression is used to reduce the sum of squared errors between true value and predicted value or residual sum of squares(RSS).\n",
    "\n",
    "Regularized regression learners are same, except they attempt to minimize RSS and some penalty for total size of coefficient values.\n",
    "\n",
    "There are 2 types of regularized regression learners:\n",
    "1) Ridge: Shrinkage penalty is a tuning hyperparameter multiplied by the **squared sum** of all coefficients.\n",
    "2) Lasso: Shrinkage penalty is a tuning hyperparameter multiplied by **sum of the absolute value** of all the coefficients.\n",
    "\n",
    "Scikit-learn includes a RidgeCV method that allows us to select the ideal  value of alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "02388751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the library\n",
    "\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7a058726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating Ridge regression with 3 alpha values\n",
    "\n",
    "regr_cv = RidgeCV(alphas = [0.1,1.0,10.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3501e61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the linear regression model\n",
    "\n",
    "model_cv = regr_cv.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "01bf5271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.8293461 ,  0.11939823, -0.26422311,  0.30398067, -0.00427544,\n",
       "       -0.03936068, -0.8937389 , -0.86433656])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the coefficients\n",
    "\n",
    "model_cv.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e4aca2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can now easily view the models alpha value:\n",
    "\n",
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f56535",
   "metadata": {},
   "source": [
    "## 13.5 Reducing features with Lasso Regrssion\n",
    "\n",
    "simplifying linear regression model by redusing the no of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7440bbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a5aa706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "\n",
    "california = fetch_california_housing()\n",
    "features = california.data\n",
    "target = california.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec8e933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardizing the features\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "df0b1677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating lasso regression with alpha value\n",
    "\n",
    "regression = Lasso(alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c4b3eebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the linear regression model\n",
    "\n",
    "model = regression.fit(features_standardized, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a87260",
   "metadata": {},
   "source": [
    "One charecteristic of Lasso regressions penalty is that it can shrink the coefficients of a model to zero, there by reducing the no of features in the model, in the above we set alpha to 0.5 and most of the model features are 0, means they are not used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "eb1e0c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29398939,  0.        ,  0.        , -0.        , -0.        ,\n",
       "       -0.        , -0.        , -0.        ])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820482c4",
   "metadata": {},
   "source": [
    "If we increase the alpha to a much higher value, we see almost none of the features are used. Increasing the alpha value to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "edffde10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0., -0., -0., -0., -0., -0.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lasso_5 = Lasso(alpha = 5)\n",
    "\n",
    "model5 = lasso_5.fit(features_standardized, target)\n",
    "model5.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24127b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
