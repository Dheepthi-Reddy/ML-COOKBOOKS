{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa204bb4",
   "metadata": {},
   "source": [
    "# CHAPTER - 20: Neural Networks\n",
    "\n",
    "At the heart of neural networks is the unit(called a node or neuron).\n",
    "\n",
    "A unit takes one more inputs, multiplies each input by a parameter(called weight), sums the weighted inputs values along with some bias value, then feeds the value into an activation function. This output is then sent forward to other neurals deeper in the neural network.\n",
    "\n",
    "Feedforward neural networks - also called multilayer perceptron - are the simplest artifical neural network used in any real-world setting.\n",
    "\n",
    "Neural network can be visualized as a series of connected layers that form a network connecting an obserevation's feature values at one end, and target value at the other end.\n",
    "\n",
    "The name feedforward comes from the fact that an observations feature values are fed \"forward\" through the network, with eac layer successively transforming the feature values with the goal that the output at the end is the same as the targe's value.\n",
    "\n",
    "Feedforward neural networks contain three types of layers of units:\n",
    "1. At the start of neural network -> input layer.(if an observation has 100 features, the input layer has 100 nodes.)\n",
    "2. At the end of the neural network -> output layer with.(transforms the o/p of the hidden layers into values useful for task at hand, like for binary classification to scale its output to 0 or 1)\n",
    "3. In between input and output layers -> hidden layers(which aren't hidden at all, these transforms the input feature values to output layers.)\n",
    "\n",
    "Neural networks with many hidden layers(eg., 10, 100, 1000) are considered ***\"deep networks\"*** and their applications are called deep learning.\n",
    "\n",
    "Neural networks are typically created with all parammeters initialized as small random values from a a Gaussian or normal uniform. Once the observations are fed through the network, the outputted value is compared with the obswervations true value using a loss function, this is called ***forward propogation***.\n",
    "\n",
    "Next an algorithm goes backwards through the network identifying how much each parameter contributed to the error between the predicted and true values, called ***backpropogation***. At each parameter the optimization algorithm determines how much each weight should be adjusted to improve the output.\n",
    "\n",
    "Neural networks learn by repeating this process of forward propogation and backpropogation for every observation in the training data multiple times, iteratively updating the values of the parameters.\n",
    "\n",
    "Each time all the observations have been sent through the network is called an ***epoch*** and training typically consists of multiple epochs.\n",
    "\n",
    "Neural networks created using Keras code can be trained using both CPUs and GPU. Whe we have larger networks and more training data, training uusing CPUs is significantly slower than training using GPUs.\n",
    "\n",
    "## 20.1 Preprocessing Data for Neural Networks\n",
    "\n",
    "Standardizing each feature using StandardScalar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f1748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9df139dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features\n",
    "features = np.array([[-100.1, 3240.1],\n",
    "                    [-200.2, -234.1],\n",
    "                    [5000.5, 150.1],\n",
    "                    [6000.6, -125.1],\n",
    "                    [9000.9, -673.1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b5e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating scalar\n",
    "\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e12d5ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the features\n",
    "\n",
    "features_standardized = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4d0ec46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.12541308,  1.96429418],\n",
       "       [-1.15329466, -0.50068741],\n",
       "       [ 0.29529406, -0.22809346],\n",
       "       [ 0.57385917, -0.42335076],\n",
       "       [ 1.40955451, -0.81216255]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show features\n",
    "\n",
    "features_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "227282c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  0\n",
      "Standard deviation:  0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean: \", round(features_standardized[:,0].mean()))\n",
    "print(\"Standard deviation: \", features_standardized[:,0].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0433b7a0",
   "metadata": {},
   "source": [
    "Typically, a neural network's parameters are initialized(i.e., created) as small random numbers. Neural networks often behavae poorly when the feature values are much larger than parameter values. Since an observations feature values are combined as they pass through individual units, it is important that all features have the same scacle. For all these reasons it is a best practice to standardize the each feature such that the faeture's values have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "## 20.2 Designing a Neural Neetwork\n",
    "\n",
    "Designing a neural network using Kera's Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd30cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the libraries\n",
    "\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845d4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a neural network\n",
    "\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24c59894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fully connected layer with a ReLU activation function\n",
    "\n",
    "network.add(layers.Dense(units=16, activation=\"relu\", input_shape=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0a4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fully connected layer with a ReLU activation function\n",
    "\n",
    "network.add((layers.Dense(units=16, activation=\"relu\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe148ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding fully connected layer with sigmoid activation function\n",
    "\n",
    "network.add(layers.Dense(units=1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e13b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the neural network\n",
    "\n",
    "network.compile(loss = \"binary_crossentropy\",  # Cross-entropy\n",
    "               optimizer = \"rmsprop\", # Root Mean Square Propogation\n",
    "               metrics=[\"accuracy\"]) #Accuracy performance metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482bc2d",
   "metadata": {},
   "source": [
    "In the above code we have a two-layer neural network, while counting the layers we don't include the input layer because it does not have any parameters to learn, using keras sequential model. \n",
    "\n",
    "Each layer is \"dense\"(fully connected) means all the units in the previous layer are connected to all the neurals in the next layer.\n",
    "\n",
    "In the first hidden layer we set units = 16 => \n",
    "\n",
    "layer contains 16 units with ReLU activation function, in keras the first hidden layer of any network must include input_shape parameter(shape of the feature), in this example it tells the first layer to expect each observation to have 10 feature values.\n",
    "\n",
    "Second layer is same as the first, without input_shape paramater.\n",
    "\n",
    "Here the network is designed for binary classification so the output layer has only one unit with sigmoid activation function, which contraints the output to between 0 and 1.\n",
    "\n",
    "Finally, we can train our model(by telling Keras how we want our network to learn) by using compile method:\n",
    "\n",
    "with optimization algorithm - RMSProp\n",
    "\n",
    "loss function - binary_crossentropy and one or more performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5012834",
   "metadata": {},
   "source": [
    "There's a lot of varieties in the types of layers and how they are combined to form the network's architecture.\n",
    "\n",
    "Selecting the right architecture is mostly an art and the topic of research.\n",
    "\n",
    "For ***feedforward neural network*** in Keras, we need to make a number of choices about:\n",
    "1. Network architecture and \n",
    "2. Training process.\n",
    "\n",
    "Each **unit*** in the hidden layers:\n",
    "1. Receives a number of inputs.\n",
    "2. Weights each input by a parameter value.\n",
    "3. Sums together all weighted inputs along with some bias(typically 1).\n",
    "4. Most often then applies some function(called an activation function).\n",
    "5. Sends the output on to units in the next layer.\n",
    "\n",
    "I:\n",
    "\n",
    "1. For each layer in the hidden and output layers we must define the no.of units to include in the layer and the activation function.\n",
    "2. More the no.of units we have in a layer, the more our network is able to learn complex patterns.\n",
    "3. More units might make network overfit the training data in a way detrimental to the performance on the test data.\n",
    "\n",
    "for hidden layers, activation function used here is the rectified linear unit(ReLU).\n",
    "\n",
    "II:\n",
    "\n",
    "1. We should then define the number of hidden layers to use in the network.\n",
    "2. More layers allow network to learn more complex relationships.\n",
    "\n",
    "III:\n",
    "\n",
    "We have to define the structure of the activation function of output layer. Some of the output layer patterns are:\n",
    "Binary Classification(one unit with sigmoid activation), Multiclass calssification(k units and a softmax activation), Regression(one unit with no activation function).\n",
    "\n",
    "IV:\n",
    "\n",
    "We then define a loss function.\n",
    "\n",
    "Binary classification => Binary cross-entropy\n",
    "Multiclass calssification => Categorical cross-entropy\n",
    "Regression => Mean square error\n",
    "\n",
    "V:\n",
    "\n",
    "Define a optimizer, this is like a strategy \"walking around\" the loss function to find the parameter values that produce the lowest error. Some of the optimizers are: stochastic gradient descent, stochastic gradient descent with momentum, root mean square propogation, and adaptive moment estimation.\n",
    "\n",
    "6:\n",
    "\n",
    "we can select one or more metrics to evaluate the performance such as accuracy.\n",
    "\n",
    "\n",
    "Keras offers two ways for creating neural networks. \n",
    "1. Keras sequential model creates neural networks by stacking together layers.\n",
    "2. By using functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f21614",
   "metadata": {},
   "source": [
    "## 20.3 Training a Binary Classifier\n",
    "\n",
    "Training a binary classifier neural network.\n",
    "Using Keras to construct a feedforward neural network and trainig using the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e13c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ac1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8c9dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting the no.of features we want\n",
    "\n",
    "number_of_features = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaaff2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data and target vector from movie review data\n",
    "\n",
    "(data_train, target_train), (data_test, target_test) = imdb.load_data(num_words = number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d5a6924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting movie review data to one-hot encoded feature matrix\n",
    "\n",
    "tokenizer = Tokenizer(num_words = number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train, mode = \"binary\")\n",
    "features_test = tokenizer.sequences_to_matrix(data_test, mode = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c90c918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting a neural network\n",
    "\n",
    "network = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "309b2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding a fully connected layer with ReLU activation function\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = \"relu\", input_shape = (number_of_features,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d14cb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fully connected layer with a ReLU activation function\n",
    "\n",
    "network.add(layers.Dense(units = 16, activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce084e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding fully connected layer with a sigmoid activation function\n",
    "\n",
    "network.add(layers.Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e451b815",
   "metadata": {},
   "outputs": [],
   "source": [
    "network.compile(loss = \"binary_crossentropy\", # Cross-entropy\n",
    "               optimizer = \"rmsprop\", # Root Mean Square Proppogation\n",
    "               metrics = [\"accuracy\"]) # Accuracy performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd80fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "250/250 [==============================] - 1s 2ms/step - loss: 0.4415 - accuracy: 0.7984 - val_loss: 0.3409 - val_accuracy: 0.8566\n",
      "Epoch 2/3\n",
      "250/250 [==============================] - 0s 892us/step - loss: 0.3262 - accuracy: 0.8635 - val_loss: 0.3338 - val_accuracy: 0.8559\n",
      "Epoch 3/3\n",
      "250/250 [==============================] - 0s 853us/step - loss: 0.3139 - accuracy: 0.8701 - val_loss: 0.3267 - val_accuracy: 0.8592\n"
     ]
    }
   ],
   "source": [
    "# training a neural network\n",
    "\n",
    "history = network.fit(features_train, # Features\n",
    "                     target_train, # Target vector\n",
    "                     epochs = 3, # No.of epochs\n",
    "                     verbose = 1, # Print description after each epoch\n",
    "                     batch_size = 100, # No.of observations per batch\n",
    "                     validation_data = (features_test, target_test)) # test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a7c67",
   "metadata": {},
   "source": [
    "Here we are training neural network with real data: 50,000 movie reviews(25,000 for training - 25,000 for testing).\n",
    "\n",
    "We convert text of reviews into 5000 binary features with 1000 most frequent words.\n",
    "\n",
    "Here the neural network uses 25000 observations each with 1000 features to predict if a movie review is positive or negative.\n",
    "\n",
    "In Keras we train our neural network using fit method. The 6 important features to define are: first 2 are features and target vector of training data.\n",
    "\n",
    "Epochs: Defines how many epochs to use when we train the data. \n",
    "\n",
    "Verbose: How much information is outputted during the training process.(0 -> no output, 1 -> outputs the progress bar, 2-> one log line per epoch)\n",
    "\n",
    "batch_size: no.of observations to propagate through the network before updating the parameters.\n",
    "\n",
    "Finally, we held a test set of data to evaluate the model, these test features and target can be given as arguments to validation_data, we can also use validation_split to define the fraction of training data we want for evaluation.\n",
    "\n",
    "In scikit-learn fit method returns a trained model, but in Keras the fit method returns a history object with loss values and performance metrics at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96b02908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a3f967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723b0fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc852ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af54e823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a681c59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93033a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa31b04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
